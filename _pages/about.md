---
permalink: /
title: "Juncheng Ma (é©¬ä¿Šç¨‹)"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# Short Bio 

I am a first-year Masterâ€™s student at the <a href='https://www.ece.pku.edu.cn/'>School of Electronic and Computer Engineering</a>, <a href='https://english.pku.edu.cn/'>Peking University </a>, advised by Prof. <a href='https://yuanli2333.github.io/'>Li Yuan</a> and Prof. <a href='https://scholar.google.com/citations?user=fn6hJx0AAAAJ&hl=en'>Yonghong Tian </a>. I am also fortunate to work closely with Prof. <a href='https://zhoudaquan.github.io/homepage.io/index.html'>Daquan Zhou</a>. I received my B.E. degree from the <a href='https://www.ucas.ac.cn/index.htm'>University of Chinese Academy of Sciences</a> (UCAS) in 2025, where I was awarded as an Outstanding Graduate. 

My research interests primarily lie in video generation for real-world applications.

# News
- â€‹*2025.05*: &nbsp;ðŸŽ‰ðŸŽ‰ â€‹I am selected as an Outstanding Graduate of Beijing, with my undergraduate thesis named an Outstanding Graduation Paper of UCAS.â€‹
- *2024.11*: &nbsp;ðŸŽ‰ðŸŽ‰ I win the China National Scholarship and First-Level Scholarship of UCAS!
- *2024.07*: &nbsp;ðŸŽ‰ðŸŽ‰ One paper is accepted by <a href='https://eccv2024.ecva.net/'>ECCV 2024</a>.


# Publications
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/omnigrid.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Omni-Grid: Taming Image-based Unified Multimodal Models to Video**

**Juncheng Ma**, Yuelin Li, Yufan Deng, Zhenyu Tang, Dongdong Yu, Li Yuan, Changhu Wang, Daquan Zhouâ€ , Yonghong Tianâ€ 

<span style="color:red">**Under Review**</span>

We present a cost-efficient approach to adapt image-based UMMs to the video domain while preserving their original strengths. Our model <span style="color:red">surpasses previous video-based UMMs on VBench</span>, while also exhibiting zero-shot multimodal interleaved abilities in video, including  CoT reasoning and text rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/cache.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**FastAvatar: Accelerating Portrait Animation via Memory-Adaptive Caching**

**Juncheng Ma**, Yuxuan Du, Yanan SUN, Zhening Xing, Changlin Li, Zhenyu Tang, Bo Li, Peng-Tao Jiang, Li Yuan, Daquan Zhouâ€ , Yonghong Tianâ€  

<span style="color:red">**Under Review**</span>

We proposed <span style="color:red">SyncCache</span>, a training-free, audio-aware, and memory-adaptive caching strategy tailored for audio-driven portrait animation, which delivers a <span style="color:red">4.12Ã—</span> acceleration on Hunyuan-Avatar and <span style="color:red">3.75Ã—</span> on Wan-S2V with negligible degradation in visual quality or audio alignment.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/teaser.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**[Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation](https://arxiv.org/abs/2407.11820)**

**Juncheng Ma**, Peiwen Sun, Yaoting Wang, Di Hu

<span style="color:red">**ECCV 2024**</span> \| [**arXiv**](https://arxiv.org/abs/2407.11820) \| [**Code**](https://github.com/GeWu-Lab/Stepping-Stones) \| [**Project**](https://gewu-lab.github.io/stepping_stones/) \| [**PDF**](pdfs/09290.pdf) \| [**Supplementary**](pdfs/09290-supp.pdf)

Rethink audio-visual semantic segmentatoin from a new perspective, with a progressive two-stage training strategy proposed to enhance the capability of <span style="color:red">audio-visual alignment</span> .
</div>
</div>

# Experiences
<div class='paper-box'><div class='paper-box-image' style="text-align: center;"><div><div class="badge"></div><img src='images/pjlab.png' alt="sym" style="display: inline-block; width: 80%;"></div></div>
<div class='paper-box-text' markdown="1">

**Research Intern, Shanghai AI Laboratory, Shanghai, China, 2024.7-2024.11**

Research on video generation especially audio-driven portrait animation.

Supervised by Dr. Yanan Sun and Dr. <a href='https://zengyh1900.github.io/'>Yanhong Zeng</a>

</div>
</div>
<div class='paper-box'><div class='paper-box-image' style="text-align: center;"><div><div class="badge"></div><img src='images/ruc.png' alt="sym" style="display: inline-block; width: 80%;"></div></div>
<div class='paper-box-text' markdown="1">

**Visiting Student, Renmin University of China, Beijing, China, 2023.10-2024.3**

Research on multimodal learning especially audio-visual segmentation, aiming to segment sound sources within a video according to its corresponding audio. 

One paper [*Stepping-Stones*](https://gewu-lab.github.io/stepping_stones/) is accepted by ECCV2024.

Supervised by Prof. <a href='https://dtaoo.github.io/'>Di Hu</a>

</div>
</div>

# Honors and Awards
- *2025*, Outstanding Graduation Paper of UCAS.
- *2025*, Outstanding Graduate of Beijing.
- *2024*, China National Scholarship.
- *2024*, First-Level Scholarship of UCAS.

# Educations
- *2025.09 - present*, Master, Peking University. 
- *2021.09 - 2025.6*, Undergraduate, University of Chinese Academy of Sciences. 




